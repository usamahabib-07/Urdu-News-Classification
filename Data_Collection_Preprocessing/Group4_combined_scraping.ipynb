{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
   "metadata": {
    "id": "a38b1de3-ba3d-4270-81e9-af7f54b5897e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21f967-ba23-447e-abf1-8e740da05e7f",
   "metadata": {
    "id": "3f21f967-ba23-447e-abf1-8e740da05e7f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import zipfile\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc3da72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4",
   "metadata": {
    "id": "23bd583c-16cf-41b0-9ad9-7ff4651e30a4"
   },
   "source": [
    "# Class Explanation: `NewsScraper`\n",
    "\n",
    "## Overview\n",
    "The `NewsScraper` class is designed for scraping news articles from three different Urdu news websites: Geo, Jang, and Express. The class has methods that cater to each site's unique structure and requirements. Below, we will go through the class and its methods, detailing what each function does, the input it takes, and the output it returns.\n",
    "\n",
    "## Class Definition\n",
    "\n",
    "```python\n",
    "class NewsScraper:\n",
    "    def __init__(self, id_=0):\n",
    "        self.id = id_\n",
    "```\n",
    "\n",
    "\n",
    "## Method 1: `get_express_articles`\n",
    "\n",
    "### Description\n",
    "Scrapes news articles from the Express website across categories like saqafat (entertainment), business, sports, science-technology, and world. The method navigates through multiple pages for each category to gather a more extensive dataset.\n",
    "\n",
    "### Input\n",
    "- **`max_pages`**: The number of pages to scrape for each category (default is 7).\n",
    "\n",
    "### Process\n",
    "- Iterates over each category and page.\n",
    "- Requests each category page and finds article cards within `<ul class='tedit-shortnews listing-page'>`.\n",
    "- Extracts the article's headline, link, and content by navigating through `<div class='horiz-news3-caption'>` and `<span class='story-text'>`.\n",
    "\n",
    "### Output\n",
    "- **Returns**: A tuple of:\n",
    "  - A Pandas DataFrame containing columns: `id`, `title`, and `link`).\n",
    "  - A dictionary `express_contents` where the key is the article ID and the value is the article content.\n",
    "\n",
    "### Data Structure\n",
    "- Article cards are identified by `<li>` tags.\n",
    "- Content is structured within `<span class='story-text'>` and `<p>` tags.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0",
   "metadata": {
    "id": "d8fc81de-6bc7-4bde-92e4-1512dcf43aa0"
   },
   "outputs": [],
   "source": [
    "class NewsScraper:\n",
    "    def __init__(self,id_=0):\n",
    "        self.id = id_\n",
    "\n",
    "\n",
    "  # write functions to scrape from other websites\n",
    "\n",
    "\n",
    "    def get_express_articles(self, max_pages=7):\n",
    "        express_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://www.express.pk'\n",
    "        categories = ['saqafat', 'business', 'sports', 'science', 'world']   # saqafat is entertainment category\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}/{category}/archives?page={page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Finding article cards\n",
    "                cards = soup.find('ul', class_='tedit-shortnews listing-page').find_all('li')  # Adjust class as per actual site structure\n",
    "                print(f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\")\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "                    try:\n",
    "                        div = card.find('div',class_='horiz-news3-caption')\n",
    "\n",
    "                        # Article Title\n",
    "                        headline = div.find('a').get_text(strip=True).replace('\\xa0', ' ')\n",
    "\n",
    "                        # Article link\n",
    "                        link = div.find('a')['href']\n",
    "\n",
    "                        # Requesting the content from each article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "\n",
    "\n",
    "                        # Content arranged in paras inside <span> tags\n",
    "                        paras = content_soup.find('span',class_='story-text').find_all('p')\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                        for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        express_df['id'].append(self.id)\n",
    "                        express_df['title'].append(headline)\n",
    "                        express_df['link'].append(link)\n",
    "                        express_df['gold_label'].append(category.replace('saqafat','entertainment').replace('science','science-technology'))\n",
    "                        express_df['content'].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\")\n",
    "\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        return pd.DataFrame(express_df)\n",
    "    \n",
    "    # each page has about 60 articles hence pages set to 3. 180 articles from each category are taken.\n",
    "    def get_geo_articles(self, max_pages=3):\n",
    "        geo_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = \"https://urdu.geo.tv/\"\n",
    "        categories = [\n",
    "            \"entertainment\",\n",
    "            \"business\",\n",
    "            \"sports\",\n",
    "            \"science-technology\",\n",
    "            \"world\",\n",
    "        ]\n",
    "\n",
    "        # Iterating over the specified number of pages\n",
    "        for category in categories:\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                url = f\"{base_url}category/{category}/{page}\"\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "                # Find all article links in the container\n",
    "                cards = soup.find_all(\"a\", class_=\"open-section\")\n",
    "                print(\n",
    "                    f\"\\t--> Found {len(cards)} articles on page {page} of '{category}'.\"\n",
    "                )\n",
    "\n",
    "                success_count = 0\n",
    "\n",
    "                for card in cards:\n",
    "\n",
    "                    try:\n",
    "                        # Article title\n",
    "                        headline = card.get(\"title\", \"\").strip()\n",
    "\n",
    "                        # Article link\n",
    "                        link = card[\"href\"]\n",
    "\n",
    "                        # Requesting the content from the article's link\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(\n",
    "                            article_response.text, \"html.parser\"\n",
    "                        )\n",
    "\n",
    "                        # Extract content inside <div class='content-area'>\n",
    "                        paras = content_soup.find(\n",
    "                            \"div\", class_=\"content-area\"\n",
    "                        ).find_all(\"p\")\n",
    "                        combined_text = \" \".join(\n",
    "                            p.get_text(strip=True)\n",
    "                            .replace(\"\\xa0\", \" \")\n",
    "                            .replace(\"\\u200b\", \"\")\n",
    "                            for p in paras\n",
    "                            if p.get_text(strip=True)\n",
    "                        )\n",
    "\n",
    "                        # Storing data\n",
    "                        geo_df[\"id\"].append(self.id)\n",
    "                        geo_df[\"title\"].append(headline)\n",
    "                        geo_df[\"link\"].append(link)\n",
    "                        geo_df[\"gold_label\"].append(\n",
    "                            category\n",
    "                        )  # 'geo' already has science-technology and entertainment so no need to replace that.\n",
    "                        geo_df[\"content\"].append(combined_text)\n",
    "\n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(\n",
    "                            f\"\\t--> Failed to scrape an article on page {page} of '{category}': {e}\"\n",
    "                        )\n",
    "\n",
    "                print(\n",
    "                    f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\"\n",
    "                )\n",
    "                print(\"\")\n",
    "\n",
    "        return pd.DataFrame(geo_df)\n",
    "    \n",
    "    def get_jang_articles(self, max_pages=7):\n",
    "        jang_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://jang.com.pk'\n",
    "        categories_dict = {\n",
    "            \"entertainment\": \"https://jang.com.pk/category/latest-news/entertainment\",\n",
    "            \"business\": \"https://jang.com.pk/category/latest-news/business\",\n",
    "            \"sports\": \"https://jang.com.pk/category/latest-news/sports\",\n",
    "            \"science-and-technology\": \"https://jang.com.pk/category/magazine/science-and-technology\",\n",
    "            \"world\": \"https://jang.com.pk/category/latest-news/world\",\n",
    "        }\n",
    "\n",
    "        for category, cat_url in categories_dict.items():\n",
    "            print(f\"Scraping category '{category}'...\")\n",
    "            success_count = 0\n",
    "\n",
    "            # Special case for Science and Technology category\n",
    "            if category == \"science-and-technology\":\n",
    "                cat_url = \"https://jang.com.pk/category/load_more_subcategories?category_id=58&parent_slug=magazine&child_slug=science-and-technology\"\n",
    "                for offset in range(0, max_pages, 1):\n",
    "                    print(f\"Scraping offset: {offset} for 'science-and-technology'...\")\n",
    "                    offset_string = f\"&offset={offset}\"\n",
    "                    final_url = cat_url + offset_string\n",
    "                    try:\n",
    "                        response = requests.get(final_url)\n",
    "                        response.raise_for_status()\n",
    "                        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                        articles = soup.select(\"li\")\n",
    "\n",
    "                        if not articles:\n",
    "                            print(f\"\\t--> No articles found at offset {offset}. Stopping.\")\n",
    "                            break\n",
    "\n",
    "                        for article in articles:\n",
    "                            try:\n",
    "                                title_tag = article.select_one('.main-heading h3')\n",
    "                                link_tag = article.find('a')\n",
    "\n",
    "                                if title_tag and link_tag:\n",
    "                                    title = title_tag.text.strip()\n",
    "                                    link = link_tag[\"href\"]\n",
    "\n",
    "                                    # print(f\"\\t--> Scraping article: {title}\")\n",
    "                                    # Request and parse the article content\n",
    "                                    article_response = requests.get(link)\n",
    "                                    article_response.raise_for_status()\n",
    "                                    content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                                    article_selector = 'body > section > div.container > div.detail-right > div.detail-content > div.description-area > div.detail_view_content'\n",
    "                                    paras = content_soup.select(f\"{article_selector} p\")\n",
    "\n",
    "                                    combined_text = \" \".join(\n",
    "                                        p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                                        for p in paras if p.get_text(strip=True)\n",
    "                                    )\n",
    "\n",
    "                                    if not title or not link or not category or not combined_text:\n",
    "                                        continue\n",
    "                                    jang_df['id'].append(self.id)\n",
    "                                    jang_df['title'].append(title)\n",
    "                                    jang_df['link'].append(link)\n",
    "                                    jang_df['gold_label'].append(\"science-technology\")\n",
    "                                    jang_df['content'].append(combined_text)\n",
    "\n",
    "                                    self.id += 1\n",
    "                                    success_count += 1\n",
    "                            except Exception as e:\n",
    "                                print(f\"\\t--> Failed to scrape an article at offset {offset}: {e}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to fetch articles at offset {offset}: {e}\")\n",
    "                        break\n",
    "\n",
    "                print(f\"Completed scraping for category 'science-and-technology'.\")\n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles of '{category}'.\")\n",
    "                continue\n",
    "\n",
    "            # Regular categories handling\n",
    "            try:\n",
    "                response = requests.get(cat_url)\n",
    "                response.raise_for_status()\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                articles = soup.select('ul.scrollPaginationNew__ > li')\n",
    "                print(f\"\\t--> Found {len(articles)} articles for '{category}'.\")\n",
    "            except Exception as e:\n",
    "                print(f\"\\t--> Failed to fetch category '{category}': {e}\")\n",
    "                continue\n",
    "\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title_tag = article.select_one('.main-heading h2')\n",
    "                    link_tag = article.select_one('.main-heading a')\n",
    "\n",
    "                    if title_tag and link_tag:\n",
    "                        title = title_tag.text.strip()\n",
    "                        link = link_tag['href']\n",
    "\n",
    "                        article_response = requests.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        content_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "                        article_selector = 'body > section > div.container > div.detail-right > div.detail-content > div.description-area > div.detail_view_content'\n",
    "                        paras = content_soup.select(f\"{article_selector} p\")\n",
    "\n",
    "                        combined_text = \" \".join(\n",
    "                            p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                            for p in paras if p.get_text(strip=True)\n",
    "                        )\n",
    "                        \n",
    "                        if not title or not link or not category or not combined_text:\n",
    "                            continue\n",
    "                        jang_df['id'].append(self.id)\n",
    "                        jang_df['title'].append(title)\n",
    "                        jang_df['link'].append(link)\n",
    "                        jang_df['gold_label'].append(category)\n",
    "                        jang_df['content'].append(combined_text)\n",
    "\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"\\t--> Failed to scrape an article of '{category}': {e}\")\n",
    "\n",
    "            print(f\"\\t--> Successfully scraped {success_count} articles of '{category}'.\")\n",
    "            print('')\n",
    "\n",
    "        jang_df['content'] = [content.replace(',', '') for content in jang_df['content']]\n",
    "        return pd.DataFrame(jang_df)\n",
    "        \n",
    "    def get_samaa_articles(self, max_pages=7):\n",
    "        samaa_df = {\n",
    "            \"id\": [],\n",
    "            \"title\": [],\n",
    "            \"link\": [],\n",
    "            \"content\": [],\n",
    "            \"gold_label\": [],\n",
    "        }\n",
    "        base_url = 'https://urdu.samaa.tv'\n",
    "        categories = {\n",
    "            'lifestyle': 'entertainment',\n",
    "            'money': 'business',\n",
    "            'sports': 'sports',\n",
    "            'tech': 'science-technology',\n",
    "            'global': 'international'\n",
    "        }\n",
    "    \n",
    "        # Define headers to mimic a real browser\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) ' \n",
    "                          'AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                          'Chrome/115.0.0.0 Safari/537.36',\n",
    "            'Accept-Language': 'en-US,en;q=0.9',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Referer': base_url\n",
    "        }\n",
    "    \n",
    "        session = requests.Session()\n",
    "        session.headers.update(headers)\n",
    "    \n",
    "        for category, gold_label in categories.items():\n",
    "            for page in range(1, max_pages + 1):\n",
    "                print(f\"Scraping page {page} of category '{category}'...\")\n",
    "                if page == 1:\n",
    "                    url = f\"{base_url}/{category}\"\n",
    "                else:\n",
    "                    url = f\"{base_url}/{category}?page={page}\"\n",
    "                \n",
    "                try:\n",
    "                    response = session.get(url)\n",
    "                    response.raise_for_status()\n",
    "                except requests.exceptions.HTTPError as http_err:\n",
    "                    print(f\"\\t--> HTTP error occurred: {http_err} for URL: {url}\")\n",
    "                    continue\n",
    "                except Exception as err:\n",
    "                    print(f\"\\t--> Other error occurred: {err} for URL: {url}\")\n",
    "                    continue\n",
    "    \n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "                #to find articles on each page\n",
    "                articles = soup.find_all('article', class_='story-article')  # Update this selector\n",
    "    \n",
    "                print(f\"\\t--> Found {len(articles)} articles on page {page} of '{category}' at {url}.\")\n",
    "    \n",
    "                success_count = 0\n",
    "    \n",
    "                for article in articles:\n",
    "                    try:\n",
    "                        # Extract the title and link\n",
    "                        a_tag = article.find('a')\n",
    "                        if not a_tag:\n",
    "                            continue\n",
    "                            \n",
    "                        # The article title was the same as image alt-text\n",
    "                        img_tag = a_tag.find('img', alt=True)\n",
    "                        if img_tag and img_tag.has_attr('alt'):\n",
    "                            title = img_tag['alt'].strip().replace('\\xa0', ' ')\n",
    "#                             print(f\"Now saving: {title}\")\n",
    "                        else:\n",
    "                            print(\"\\t--> <img> tag with 'alt' attribute not found in <a> tag; skipping.\")\n",
    "                            continue  # Skip this article if <img> tag or 'alt' attribute is missing\n",
    "\n",
    "                        \n",
    "                        link = a_tag['href']\n",
    "                        if not link.startswith('http'):\n",
    "                            link = base_url + link\n",
    "    \n",
    "                        # Fetch the article content\n",
    "                        article_response = session.get(link)\n",
    "                        article_response.raise_for_status()\n",
    "                        article_soup = BeautifulSoup(article_response.text, \"html.parser\")\n",
    "    \n",
    "                        # To find the main content within each article\n",
    "                        content_div = article_soup.find('div', class_='article-content')  # Update this selector\n",
    "                        if not content_div:\n",
    "                            raise ValueError(\"Content div not found\")\n",
    "    \n",
    "                        paragraphs = content_div.find_all('p')\n",
    "                        combined_text = \" \".join(\n",
    "                            p.get_text(strip=True).replace('\\xa0', ' ').replace('\\u200b', '')\n",
    "                            for p in paragraphs if p.get_text(strip=True)\n",
    "                        )\n",
    "    \n",
    "                        # Store the data\n",
    "                        samaa_df['id'].append(self.id)\n",
    "                        samaa_df['title'].append(title)\n",
    "                        samaa_df['link'].append(link)\n",
    "                        samaa_df['gold_label'].append(gold_label)\n",
    "                        samaa_df['content'].append(combined_text)\n",
    "    \n",
    "                        # Increment ID and success count\n",
    "                        self.id += 1\n",
    "                        success_count += 1\n",
    "    \n",
    "                        # To be polite to the server, sleep for a random short duration\n",
    "                        time.sleep(random.uniform(0.5, 1.5))\n",
    "    \n",
    "                    except Exception as e:\n",
    "                        print(f\"\\t--> Failed to scrape an article on page {page} of '{category}', Error: {e}\")\n",
    "    \n",
    "                print(f\"\\t--> Successfully scraped {success_count} articles from page {page} of '{category}'.\")\n",
    "            print('')  # Add a newline for better readability between categories\n",
    "    \n",
    "        return pd.DataFrame(samaa_df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1",
   "metadata": {
    "id": "e9a8ad94-10b0-4458-bb7f-3402eecd80d1"
   },
   "outputs": [],
   "source": [
    "scraper = NewsScraper()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dfab3f",
   "metadata": {},
   "source": [
    "### Create DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85",
   "metadata": {
    "id": "321373e7-8ef4-468f-81d0-8be61fe2ba85"
   },
   "outputs": [],
   "source": [
    "#max_pages has been set to 1 here for quick testing\n",
    "#we scraped with ranges between 5 - 10 depending on how many articles were on one page for each publisher\n",
    "\n",
    "print(\"Now scraping Express: \")\n",
    "express_df = scraper.get_express_articles(max_pages=1)\n",
    "print(\"Now scraping Geo: \")\n",
    "geo_df = scraper.get_geo_articles(max_pages=1)\n",
    "print(\"Now scraping Jang: \")\n",
    "jang_df = scraper.get_jang_articles(max_pages=1)\n",
    "print(\"Now scraping Samaa: \")\n",
    "samaa_df = scraper.get_samaa_articles(max_pages=1)\n",
    "print(\"Scraping complete. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nn7TyroayZhg",
   "metadata": {
    "id": "nn7TyroayZhg"
   },
   "source": [
    "# Output\n",
    "- Save a combined csv of all 3 sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668040f6-1f3b-4400-8daa-39b1296a151e",
   "metadata": {
    "id": "668040f6-1f3b-4400-8daa-39b1296a151e"
   },
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "df_list.append(express_df)\n",
    "df_list.append(geo_df)\n",
    "df_list.append(jang_df)\n",
    "df.list.append(samaa_df)\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "combined_df.to_excel('all_data.csv', index=False)\n",
    "print(\"All data has been saved as all_data.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
