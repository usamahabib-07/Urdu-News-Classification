{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Urdu Text Data Preprocessing\n",
    "\n",
    "### **Description**  \n",
    "This notebook has the data cleaning and pre processing steps which formulated the final excel file that all other models have used to formulate their data. After pre-processing was done, the data was stored into an excel file named \"normalized_and_tokenized_combined_data.xlsx\" from where the data will be used for model workings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Imports ####\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import urduhack\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "##### Globals #####\n",
    "nlp = spacy.blank('ur') #nlp model which helps tokenize urdu words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>’کبھی میں کبھی تم‘ کی اداکارہ نائمہ بٹ کو سوشل...</td>\n",
       "      <td>https://www.express.pk/story/2732145/kabhi-mai...</td>\n",
       "      <td>مشہور ڈرامے ’کبھی میں کبھی تم‘ میں منفی کردار ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>’سدھو موسے والا واپس آگیا‘ گلوکار کے ننھے بھا...</td>\n",
       "      <td>https://www.express.pk/story/2732131/sidhu-moo...</td>\n",
       "      <td>بھارت کے آنجہانی پنجابی گلوکار سدھو موسے والا...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>ترکیہ کے ’سلطان صلاح الدین ایوبی‘ اسٹار کا پاک...</td>\n",
       "      <td>https://www.express.pk/story/2732124/turkey-k-...</td>\n",
       "      <td>ترکی کے اداکار اوغور گونیش کی سیریز ’سلطان صلا...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>نعمان اعجاز اور صبا قمر بولڈ مناظر فلمانے پر ت...</td>\n",
       "      <td>https://www.express.pk/story/2732113/nauman-ej...</td>\n",
       "      <td>نعمان اعجاز اور صبا قمر کی ویب سیریز کے بولڈ س...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>سلمان خان اور لارنس بشنوئی پر گانا لکھنے والا ...</td>\n",
       "      <td>https://www.express.pk/story/2732103/salman-kh...</td>\n",
       "      <td>بالی ووڈ کے سپر اسٹار سلمان خان کو ایک مرتبہ پ...</td>\n",
       "      <td>entertainment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              title  \\\n",
       "0   0  ’کبھی میں کبھی تم‘ کی اداکارہ نائمہ بٹ کو سوشل...   \n",
       "1   1  ’سدھو موسے والا واپس آگیا‘ گلوکار کے ننھے بھا...   \n",
       "2   2  ترکیہ کے ’سلطان صلاح الدین ایوبی‘ اسٹار کا پاک...   \n",
       "3   3  نعمان اعجاز اور صبا قمر بولڈ مناظر فلمانے پر ت...   \n",
       "4   4  سلمان خان اور لارنس بشنوئی پر گانا لکھنے والا ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.express.pk/story/2732145/kabhi-mai...   \n",
       "1  https://www.express.pk/story/2732131/sidhu-moo...   \n",
       "2  https://www.express.pk/story/2732124/turkey-k-...   \n",
       "3  https://www.express.pk/story/2732113/nauman-ej...   \n",
       "4  https://www.express.pk/story/2732103/salman-kh...   \n",
       "\n",
       "                                             content     gold_label  \n",
       "0  مشہور ڈرامے ’کبھی میں کبھی تم‘ میں منفی کردار ...  entertainment  \n",
       "1  بھارت کے آنجہانی پنجابی گلوکار سدھو موسے والا...  entertainment  \n",
       "2  ترکی کے اداکار اوغور گونیش کی سیریز ’سلطان صلا...  entertainment  \n",
       "3  نعمان اعجاز اور صبا قمر کی ویب سیریز کے بولڈ س...  entertainment  \n",
       "4  بالی ووڈ کے سپر اسٹار سلمان خان کو ایک مرتبہ پ...  entertainment  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_excel('../Data/combined_data.xlsx')\n",
    "\n",
    "# Display the first few rows to understand the data\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checking for Missing Values and Duplicates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any rows with missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2281, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any rows with duplicate values: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2281, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if the gold_labels are all in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entertainment' 'business' 'sports' 'science-technology' 'world'\n",
      " 'international']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# Check the unique values in the 'gold_label' column\n",
    "unique_labels = df['gold_label'].unique()\n",
    "\n",
    "# Print the unique labels\n",
    "print(unique_labels)\n",
    "print(len(unique_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### `world` is not what we want, need to map it to `international`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['entertainment' 'business' 'sports' 'science-technology' 'international']\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "df['gold_label'] = df['gold_label'].replace('world', 'international')\n",
    "\n",
    "#verifying:\n",
    "unique_labels_after_mapping = df['gold_label'].unique()\n",
    "print(unique_labels_after_mapping)\n",
    "\n",
    "print(len(df['gold_label'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data types of each column\n",
      " id             int64\n",
      "title         object\n",
      "link          object\n",
      "content       object\n",
      "gold_label    object\n",
      "dtype: object\n",
      "\n",
      "Missing values in each column: \n",
      " id            0\n",
      "title         0\n",
      "link          0\n",
      "content       0\n",
      "gold_label    0\n",
      "dtype: int64\n",
      "\n",
      "Duplicated rows: 0\n"
     ]
    }
   ],
   "source": [
    "# Check the data types of each column\n",
    "print(\"data types of each column\\n\",df.dtypes)\n",
    "\n",
    "# Check for missing values in each column\n",
    "print(\"\\nMissing values in each column: \\n\",df.isnull().sum())\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nDuplicated rows:\",df.duplicated().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline for Urdu Text Data\n",
    "\n",
    "> **Note:** This pipeline assumes data was cleaned beforehand. Which we have done above.\n",
    "\n",
    "Steps to prepare Urdu text data for machine learning models:\n",
    "\n",
    "1. **Shuffle the Data:**  \n",
    "   Randomize the order of the data samples to eliminate any unintended patterns that might bias the model.\n",
    "\n",
    "2. **Text Normalization:**  \n",
    "   Use the **UrduHack library** to normalize Urdu text by addressing inconsistencies in script, such as fixing spacing, removing unnecessary diacritics, and standardizing character forms.\n",
    "\n",
    "3. **Tokenization:**  \n",
    "   Split the normalized text into **word tokens** using the **SpaCy library**, which provides efficient tokenization specifically designed for the Urdu language.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Classes and Functions Required for Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Test Split Function\n",
    "def train_test_split(X, y, test_size=0.2, random_state=None):\n",
    "    if random_state is not None:\n",
    "        np.random.seed(random_state)\n",
    "    \n",
    "    n_samples = len(X)\n",
    "    indices = np.random.permutation(n_samples)\n",
    "    test_size = int(test_size * n_samples)\n",
    "    \n",
    "    test_indices = indices[:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    return (\n",
    "        X[train_indices], X[test_indices],\n",
    "        y[train_indices], y[test_indices]\n",
    "    )\n",
    "\n",
    "def normalization_and_tokenization(text):\n",
    "\n",
    "    # Normalize text (handles things like normalization of characters)\n",
    "    text = urduhack.normalization.normalize(text)\n",
    "        \n",
    "    # Tokenize text into words using Urdu tokenizer from Spacy\n",
    "    tokens = nlp(text) #nlp is a global variable (a model from spacy) declared where imports are.\n",
    "    \n",
    "    return [token.text for token in tokens if len(token.text) > 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pre-Processing actually happens here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Shuffling the data\n",
    "df = df.sample(frac=1, random_state=20).reset_index(drop=True)\n",
    "\n",
    "# Normalizing and Tokenizing the 'content' column\n",
    "df['processed_content'] = df['content'].apply(normalization_and_tokenization)\n",
    "\n",
    "\n",
    "# Convert lists of tokens back to text so we can apply TF-IDF\n",
    "df['processed_content'] = df['processed_content'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save changes to file so models can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are saving this file and all models will use this.\n",
    "df.to_excel(\"normalized_and_tokenized_combined_data.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>content</th>\n",
       "      <th>gold_label</th>\n",
       "      <th>processed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>780</td>\n",
       "      <td>امریکی صدارتی الیکشن، کونسی ریاستیں ریپبلکنز ن...</td>\n",
       "      <td>https://urdu.geo.tv/latest/385845-</td>\n",
       "      <td>امریکی صدارتی الیکشن میں ری پبلکنز امیدوار ڈون...</td>\n",
       "      <td>world</td>\n",
       "      <td>[امریکی, صدارتی, الیکشن, میں, ری, پبلکنز, امید...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>انروا پر اسرائیلی پابندی غزہ کے بچوں کے قتل عا...</td>\n",
       "      <td>https://urdu.geo.tv/latest/384924-</td>\n",
       "      <td>اقوام متحدہ کے ادارہ برائے اطفال (یونیسیف)  نے...</td>\n",
       "      <td>world</td>\n",
       "      <td>[اقوام, متحدہ, کے, ادارہ, برائے, اطفال, یونیسی...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>574</td>\n",
       "      <td>میٹا کا گوگل کے مقابلے میں اپنا اے آئی سرچ انج...</td>\n",
       "      <td>https://urdu.geo.tv/latest/384923-</td>\n",
       "      <td>میٹا کی جانب سے آرٹی فیشل انٹیلی جنس (اے آئی) ...</td>\n",
       "      <td>science-technology</td>\n",
       "      <td>[میٹا, کی, جانب, سے, آرٹی, فیشل, انٹیلی, جنس, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>200</td>\n",
       "      <td>کراچی کے سوا باقی ملک کیلئے بجلی سستی ہوگئی</td>\n",
       "      <td>https://urdu.geo.tv/latest/385886-</td>\n",
       "      <td>اسلام آباد:کراچی کے سوا باقی ملک کے لیے بجلی س...</td>\n",
       "      <td>business</td>\n",
       "      <td>[اسلام, آباد, کراچی, کے, سوا, باقی, ملک, کے, ل...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>575</td>\n",
       "      <td>سرکاری ملازمین کا بیٹا چین کا امیر ترین شخص کی...</td>\n",
       "      <td>https://urdu.geo.tv/latest/384908-</td>\n",
       "      <td>ٹک ٹاک کی سرپرست کمپنی بائیٹ ڈانس کے بانی زینگ...</td>\n",
       "      <td>science-technology</td>\n",
       "      <td>[ٹک, ٹاک, کی, سرپرست, کمپنی, بائیٹ, ڈانس, کے, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                              title  \\\n",
       "0  780  امریکی صدارتی الیکشن، کونسی ریاستیں ریپبلکنز ن...   \n",
       "1  893  انروا پر اسرائیلی پابندی غزہ کے بچوں کے قتل عا...   \n",
       "2  574  میٹا کا گوگل کے مقابلے میں اپنا اے آئی سرچ انج...   \n",
       "3  200        کراچی کے سوا باقی ملک کیلئے بجلی سستی ہوگئی   \n",
       "4  575  سرکاری ملازمین کا بیٹا چین کا امیر ترین شخص کی...   \n",
       "\n",
       "                                 link  \\\n",
       "0  https://urdu.geo.tv/latest/385845-   \n",
       "1  https://urdu.geo.tv/latest/384924-   \n",
       "2  https://urdu.geo.tv/latest/384923-   \n",
       "3  https://urdu.geo.tv/latest/385886-   \n",
       "4  https://urdu.geo.tv/latest/384908-   \n",
       "\n",
       "                                             content          gold_label  \\\n",
       "0  امریکی صدارتی الیکشن میں ری پبلکنز امیدوار ڈون...               world   \n",
       "1  اقوام متحدہ کے ادارہ برائے اطفال (یونیسیف)  نے...               world   \n",
       "2  میٹا کی جانب سے آرٹی فیشل انٹیلی جنس (اے آئی) ...  science-technology   \n",
       "3  اسلام آباد:کراچی کے سوا باقی ملک کے لیے بجلی س...            business   \n",
       "4  ٹک ٹاک کی سرپرست کمپنی بائیٹ ڈانس کے بانی زینگ...  science-technology   \n",
       "\n",
       "                                   processed_content  \n",
       "0  [امریکی, صدارتی, الیکشن, میں, ری, پبلکنز, امید...  \n",
       "1  [اقوام, متحدہ, کے, ادارہ, برائے, اطفال, یونیسی...  \n",
       "2  [میٹا, کی, جانب, سے, آرٹی, فیشل, انٹیلی, جنس, ...  \n",
       "3  [اسلام, آباد, کراچی, کے, سوا, باقی, ملک, کے, ل...  \n",
       "4  [ٹک, ٹاک, کی, سرپرست, کمپنی, بائیٹ, ڈانس, کے, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Sanity Check\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
